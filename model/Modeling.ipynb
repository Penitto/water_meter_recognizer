{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd0a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43983eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrappers\n",
    "intersect = list(set(os.listdir('./TlkWaterMeters/images')) & set(os.listdir('./TlkWaterMeters/masks')))\n",
    "train, test = train_test_split(intersect, random_state=57)\n",
    "\n",
    "IMAGES_DIR = './TlkWaterMeters/images'\n",
    "MASKS_DIR = './TlkWaterMeters/masks'\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "class WaterMeter(Dataset):\n",
    "    def __init__(self, objects, image_dir, target_dir, transform=None):\n",
    "                \n",
    "        self.image_paths = sorted([os.path.join(image_dir, file) for file in objects])\n",
    "        self.target_paths = sorted([os.path.join(target_dir, file) for file in objects])\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_path = self.image_paths[idx]\n",
    "        target_path = self.target_paths[idx]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(target_path)\n",
    "        \n",
    "        # Правим косяки разметки\n",
    "        mask[mask <= 127] = 0\n",
    "        mask[mask > 127] = 255\n",
    "        \n",
    "        # Первый слой маски - класс \"цифры\"\n",
    "        # Второй слой маски - класс \"не цифры\"\n",
    "        mask[..., 1] = 255 - mask[..., 1] \n",
    "        \n",
    "        mask = mask / 255.\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            return transformed['image'], transformed['mask'][:2, ...]\n",
    "        else:\n",
    "            return image, mask[:2, ...]\n",
    "        \n",
    "train_transform = A.Compose([\n",
    "    A.ColorJitter(), \n",
    "    A.Rotate(),\n",
    "    A.geometric.Resize(height=HEIGHT, width=WIDTH),\n",
    "    A.Normalize(always_apply=True),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.geometric.Resize(height=HEIGHT, width=WIDTH),\n",
    "    A.Normalize(always_apply=True),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "train_dataset = WaterMeter(\n",
    "    objects=train,\n",
    "    image_dir=IMAGES_DIR, \n",
    "    target_dir=MASKS_DIR, \n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "test_dataset = WaterMeter(\n",
    "    objects=test,\n",
    "    image_dir=IMAGES_DIR, \n",
    "    target_dir=MASKS_DIR, \n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = train_dataset[0]\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1, ncols=3, figsize=(16,9))\n",
    "\n",
    "to_show = [image, mask[:1,...], mask[1:,...]]\n",
    "\n",
    "for i, img in enumerate(to_show):\n",
    "    ax[i].imshow(img.numpy().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9808694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard setup\n",
    "time = datetime.now()\n",
    "writer = SummaryWriter(log_dir=f'log_{time.strftime(\"%Y%M%D_%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=2,                      # model output channels (number of classes in your dataset)\n",
    "    activation='softmax'\n",
    ")\n",
    "model = model.float()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c727fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "for i in range(EPOCHS):\n",
    "    print('EPOCH: ', i)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X.float())\n",
    "        output = loss(pred, y.long().argmax(dim=1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            output, current = output.item(), batch * len(X)\n",
    "            writer.add_scalar('train loss ', output, i * len(X) + batch)\n",
    "#             writer.add_figure()\n",
    "            print(f\"Train loss: {output:>7f}  [{current:>5d}/{len(train_dataloader.dataset):>5d}]\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X.float()).detach()\n",
    "            test_loss += loss(pred, y.long().argmax(dim=1)).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item() / pred.shape[0]\n",
    "    test_loss /= len(test_dataloader)\n",
    "    correct /= len(test_dataloader.dataset)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ba131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_file(model, image_transform, image_path):\n",
    "    model.eval()\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image_transform(image=image)['image']\n",
    "    image = image.to(device)\n",
    "    pred = model(image.unsqueeze(0).float())\n",
    "    pred[pred >= 0.5] = 1\n",
    "    pred[pred < 0.5] = 0\n",
    "    pred = pred.squeeze().cpu().data\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './photo_2022-07-12 17.53.56.jpeg'\n",
    "image = test_transform(image=cv2.imread(image_path))['image']\n",
    "mask = inference_file(model, test_transform, image_path)\n",
    "fig,ax = plt.subplots(nrows=1, ncols=3, figsize=(16,9))\n",
    "\n",
    "to_show = [image, mask[:1,...], mask[1:,...]]\n",
    "\n",
    "for i, img in enumerate(to_show):\n",
    "    ax[i].imshow(img.numpy().transpose())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64f5201b89a3db0715a052293125473bd94c43d58e610d6789b4e7ef24eb3db8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
